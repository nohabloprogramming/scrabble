{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wallace4x4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVTxlzpAovcg+jp+V0rcqn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zainkhan-afk/scrabble/blob/master/wallace4x4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pqeFuyfiVKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1n3Kn_a2ZVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              ])\n",
        "\n",
        "transformTest = transforms.Compose([transforms.ToTensor(),\n",
        "                              ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fvOvpLH2it7",
        "colab_type": "code",
        "outputId": "379777a6-7c7b-4754-b520-e483213d7374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "X = np.zeros([16, 1])\n",
        "multiplicand = np.zeros([8, 1])\n",
        "multiplier = np.zeros([8, 1])\n",
        "for i in range(0, 2):\n",
        "  for j in range(0, 2):\n",
        "    bin = format(i, \"b\")\n",
        "data = multiplicand\n",
        "data = np.append(data, multiplier, 0)\n",
        "X = np.append(X, data, 1)\n",
        "print(multiplier.shape)\n",
        "print(data.shape)\n",
        "print(X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 1)\n",
            "(16, 1)\n",
            "(16, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndEwMzgYclaX",
        "colab_type": "text"
      },
      "source": [
        "**Loading the Training Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjO5GI8lSRWY",
        "colab_type": "code",
        "outputId": "4ec043a5-5476-4b4a-c6a3-568b94bc5136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "Data = np.loadtxt('NoisyXORTrainingData.txt')\n",
        "Y = np.zeros([8, 1])\n",
        "Data = Data[:, :8]\n",
        "print(Data.shape)\n",
        "\n",
        "for d in range(0, 5000):\n",
        "  multiplicand = 0\n",
        "  multiplier = 0\n",
        "\n",
        "  for i in range(0, 4):\n",
        "    multiplicand = multiplicand + Data[d, i]*2**i\n",
        "    multiplier = multiplier +  Data[d, i+4]*2**i\n",
        "\n",
        "  result = multiplicand*multiplier\n",
        "  resultBin = format(int(result), \"b\")\n",
        "  opLen = len(resultBin)\n",
        "  op = []\n",
        "  for i in range(0, 8):\n",
        "    if i<opLen:\n",
        "      op.append(int(resultBin[i]))\n",
        "    else:\n",
        "      op.append(0)\n",
        "  op = np.array(op)\n",
        "  op = op[:, np.newaxis]\n",
        "  Y = np.append(Y, op, 1)\n",
        "\n",
        "Y = np.transpose(Y)\n",
        "Y = Y[1:, :]\n",
        "print(Y.shape)\n",
        "print(Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 8)\n",
            "(5000, 8)\n",
            "[[1. 1. 0. ... 0. 0. 0.]\n",
            " [1. 1. 0. ... 1. 0. 0.]\n",
            " [1. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 1. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzjGZXTwcrJS",
        "colab_type": "text"
      },
      "source": [
        "**Loading the Testing Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZlcyFtDcvsf",
        "colab_type": "code",
        "outputId": "8216184c-811e-47b6-9e27-11e291222055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "DataTest = np.loadtxt('NoisyXORTestData.txt')\n",
        "YTest = np.zeros([8, 1])\n",
        "DataTest = DataTest[:, :8]\n",
        "print(DataTest.shape)\n",
        "\n",
        "for d in range(0, 5000):\n",
        "  multiplicand = 0\n",
        "  multiplier = 0\n",
        "\n",
        "  for i in range(0, 4):\n",
        "    multiplicand = multiplicand + DataTest[d, i]*2**i\n",
        "    multiplier = multiplier +  DataTest[d, i+4]*2**i\n",
        "\n",
        "  result = multiplicand*multiplier\n",
        "  resultBin = format(int(result), \"b\")\n",
        "  opLen = len(resultBin)\n",
        "  op = []\n",
        "  for i in range(0, 8):\n",
        "    if i<opLen:\n",
        "      op.append(int(resultBin[i]))\n",
        "    else:\n",
        "      op.append(0)\n",
        "  op = np.array(op)\n",
        "  op = op[:, np.newaxis]\n",
        "  YTest = np.append(YTest, op, 1)\n",
        "\n",
        "YTest = np.transpose(YTest)\n",
        "YTest = YTest[1:, :]\n",
        "print(YTest.shape)\n",
        "print(YTest)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 8)\n",
            "(5000, 8)\n",
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 1. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 1. ... 1. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW7N3QPaaFWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(Data, batch_size=64, shuffle=False)\n",
        "labels = torch.utils.data.DataLoader(Y, batch_size=64, shuffle=False)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(DataTest, batch_size=64, shuffle=False)\n",
        "labelsTest = torch.utils.data.DataLoader(YTest, batch_size=64, shuffle=False)\n",
        "\n",
        "# dataiter = iter(trainloader)\n",
        "# print(dataiter.shape)\n",
        "# images= dataiter.next()\n",
        "# print(type(images))\n",
        "# print(images.shape)\n",
        "\n",
        "# Yiter = iter(labels)\n",
        "# labs = Yiter.next()\n",
        "# print(labs.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAvDy8srbO-w",
        "colab_type": "code",
        "outputId": "7d721e23-5498-4eaa-a6c9-ec7902dc3721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "input_size = 8\n",
        "hidden_sizes = [128, 64]\n",
        "output_size = 8\n",
        "\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size),\n",
        "                      nn.Sigmoid())\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=8, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (5): ReLU()\n",
            "  (6): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (7): ReLU()\n",
            "  (8): Linear(in_features=64, out_features=8, bias=True)\n",
            "  (9): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecK831V1bbUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
        "time0 = time()\n",
        "epochs = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jBtbw_NbcCN",
        "colab_type": "code",
        "outputId": "6aedf132-1a43-47cd-e624-d37a6774ac9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    Yiter = iter(labels)\n",
        "    for images in trainloader:\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        # images = images.view(images.shape[0], -1)\n",
        "        labs = Yiter.next()\n",
        "        # Training pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # print(\"IP: \", images.shape)\n",
        "        output = model(images.float())\n",
        "        # print(\"OP: \", output.shape)\n",
        "        # print(\"Truth: \", labs.shape)\n",
        "        loss = criterion(output, labs.float())\n",
        "        \n",
        "        #This is where the model learns by backpropagating\n",
        "        loss.backward()\n",
        "        \n",
        "        #And optimizes its weights here\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    # else:\n",
        "    print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
        "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - Training loss: 0.2062606092872499\n",
            "Epoch 1 - Training loss: 0.16672549085526528\n",
            "Epoch 2 - Training loss: 0.1651590130751646\n",
            "Epoch 3 - Training loss: 0.16452400612680218\n",
            "Epoch 4 - Training loss: 0.16372081505346903\n",
            "Epoch 5 - Training loss: 0.16234105240695085\n",
            "Epoch 6 - Training loss: 0.15945697245718557\n",
            "Epoch 7 - Training loss: 0.1530823656652547\n",
            "Epoch 8 - Training loss: 0.14494312149059924\n",
            "Epoch 9 - Training loss: 0.13967094919349574\n",
            "Epoch 10 - Training loss: 0.13327113359789305\n",
            "Epoch 11 - Training loss: 0.12644114741418935\n",
            "Epoch 12 - Training loss: 0.11996359849655175\n",
            "Epoch 13 - Training loss: 0.11368234983727901\n",
            "Epoch 14 - Training loss: 0.10810346920279008\n",
            "Epoch 15 - Training loss: 0.10325329703620717\n",
            "Epoch 16 - Training loss: 0.09860707347906089\n",
            "Epoch 17 - Training loss: 0.09390196279634404\n",
            "Epoch 18 - Training loss: 0.08921563568749005\n",
            "Epoch 19 - Training loss: 0.08495094976093195\n",
            "Epoch 20 - Training loss: 0.0814338847051693\n",
            "Epoch 21 - Training loss: 0.07861353362662883\n",
            "Epoch 22 - Training loss: 0.07624905980840514\n",
            "Epoch 23 - Training loss: 0.0740909173235863\n",
            "Epoch 24 - Training loss: 0.07196959763576713\n",
            "Epoch 25 - Training loss: 0.06984981611559662\n",
            "Epoch 26 - Training loss: 0.06783409072439882\n",
            "Epoch 27 - Training loss: 0.06580479259166537\n",
            "Epoch 28 - Training loss: 0.06383822985653635\n",
            "Epoch 29 - Training loss: 0.06194177227495592\n",
            "Epoch 30 - Training loss: 0.06018644557157649\n",
            "Epoch 31 - Training loss: 0.058501750277944756\n",
            "Epoch 32 - Training loss: 0.056845534027000016\n",
            "Epoch 33 - Training loss: 0.05524424364483809\n",
            "Epoch 34 - Training loss: 0.05362160418984256\n",
            "Epoch 35 - Training loss: 0.05183492294406589\n",
            "Epoch 36 - Training loss: 0.050002319667535496\n",
            "Epoch 37 - Training loss: 0.047991262017926084\n",
            "Epoch 38 - Training loss: 0.04590146507643446\n",
            "Epoch 39 - Training loss: 0.04386307002057003\n",
            "Epoch 40 - Training loss: 0.04203323701608785\n",
            "Epoch 41 - Training loss: 0.0404020468645458\n",
            "Epoch 42 - Training loss: 0.03880035273636444\n",
            "Epoch 43 - Training loss: 0.037148716942041735\n",
            "Epoch 44 - Training loss: 0.035367363120747515\n",
            "Epoch 45 - Training loss: 0.03381181277240379\n",
            "Epoch 46 - Training loss: 0.03239671702060518\n",
            "Epoch 47 - Training loss: 0.031042648667964755\n",
            "Epoch 48 - Training loss: 0.029872586266903936\n",
            "Epoch 49 - Training loss: 0.0288188160880457\n",
            "Epoch 50 - Training loss: 0.027895499327326122\n",
            "Epoch 51 - Training loss: 0.027024983322318598\n",
            "Epoch 52 - Training loss: 0.0262686421953236\n",
            "Epoch 53 - Training loss: 0.02562267060945683\n",
            "Epoch 54 - Training loss: 0.024938227467333214\n",
            "Epoch 55 - Training loss: 0.024315086240538315\n",
            "Epoch 56 - Training loss: 0.0237295477167715\n",
            "Epoch 57 - Training loss: 0.023230210571443734\n",
            "Epoch 58 - Training loss: 0.022666311504531512\n",
            "Epoch 59 - Training loss: 0.022087879195997986\n",
            "Epoch 60 - Training loss: 0.021593554275511188\n",
            "Epoch 61 - Training loss: 0.02118472258642882\n",
            "Epoch 62 - Training loss: 0.02079379401770951\n",
            "Epoch 63 - Training loss: 0.020381621226574047\n",
            "Epoch 64 - Training loss: 0.02001442775554672\n",
            "Epoch 65 - Training loss: 0.019665647698919986\n",
            "Epoch 66 - Training loss: 0.01924773073413327\n",
            "Epoch 67 - Training loss: 0.018827745296013883\n",
            "Epoch 68 - Training loss: 0.01850052474396704\n",
            "Epoch 69 - Training loss: 0.018172276719131426\n",
            "Epoch 70 - Training loss: 0.017871076848263605\n",
            "Epoch 71 - Training loss: 0.017583158645260184\n",
            "Epoch 72 - Training loss: 0.01728404659329902\n",
            "Epoch 73 - Training loss: 0.016993929242974597\n",
            "Epoch 74 - Training loss: 0.01674584520957138\n",
            "Epoch 75 - Training loss: 0.016519619994855756\n",
            "Epoch 76 - Training loss: 0.016303715997528802\n",
            "Epoch 77 - Training loss: 0.016082659416796662\n",
            "Epoch 78 - Training loss: 0.015871679817055222\n",
            "Epoch 79 - Training loss: 0.015662610542642164\n",
            "Epoch 80 - Training loss: 0.015465175544347945\n",
            "Epoch 81 - Training loss: 0.015274590588634528\n",
            "Epoch 82 - Training loss: 0.015093457938136556\n",
            "Epoch 83 - Training loss: 0.014931024635752922\n",
            "Epoch 84 - Training loss: 0.014783642985681189\n",
            "Epoch 85 - Training loss: 0.014647338492206381\n",
            "Epoch 86 - Training loss: 0.014526049311779723\n",
            "Epoch 87 - Training loss: 0.014411518143937934\n",
            "Epoch 88 - Training loss: 0.0143110779397001\n",
            "Epoch 89 - Training loss: 0.01422126160321545\n",
            "Epoch 90 - Training loss: 0.014141878773328623\n",
            "Epoch 91 - Training loss: 0.01407059967588586\n",
            "Epoch 92 - Training loss: 0.014007622900690082\n",
            "Epoch 93 - Training loss: 0.013950849317391462\n",
            "Epoch 94 - Training loss: 0.01390204035759538\n",
            "Epoch 95 - Training loss: 0.013857961437794605\n",
            "Epoch 96 - Training loss: 0.01382081582552836\n",
            "Epoch 97 - Training loss: 0.013785869643516555\n",
            "Epoch 98 - Training loss: 0.013755605561965251\n",
            "Epoch 99 - Training loss: 0.013729182820578542\n",
            "Epoch 100 - Training loss: 0.013704350224071288\n",
            "Epoch 101 - Training loss: 0.013682092504599426\n",
            "Epoch 102 - Training loss: 0.01366164502275141\n",
            "Epoch 103 - Training loss: 0.013642930867787026\n",
            "Epoch 104 - Training loss: 0.013626267290615205\n",
            "Epoch 105 - Training loss: 0.013611246430892733\n",
            "Epoch 106 - Training loss: 0.01359762477723858\n",
            "Epoch 107 - Training loss: 0.01358508443624913\n",
            "Epoch 108 - Training loss: 0.013573489723656374\n",
            "Epoch 109 - Training loss: 0.01356264953560467\n",
            "Epoch 110 - Training loss: 0.013552889634583946\n",
            "Epoch 111 - Training loss: 0.013543506636272502\n",
            "Epoch 112 - Training loss: 0.013534885224049227\n",
            "Epoch 113 - Training loss: 0.013526576134977462\n",
            "Epoch 114 - Training loss: 0.013519021865169081\n",
            "Epoch 115 - Training loss: 0.013511815238036688\n",
            "Epoch 116 - Training loss: 0.013505142476834073\n",
            "Epoch 117 - Training loss: 0.013498701249496847\n",
            "Epoch 118 - Training loss: 0.013492774041487446\n",
            "Epoch 119 - Training loss: 0.013487188228040556\n",
            "Epoch 120 - Training loss: 0.01348182748225105\n",
            "Epoch 121 - Training loss: 0.0134767835865481\n",
            "Epoch 122 - Training loss: 0.013472051636752072\n",
            "Epoch 123 - Training loss: 0.013467541907595682\n",
            "Epoch 124 - Training loss: 0.013463296615106018\n",
            "Epoch 125 - Training loss: 0.013459194905442905\n",
            "Epoch 126 - Training loss: 0.013455337738688988\n",
            "Epoch 127 - Training loss: 0.013451711704978083\n",
            "Epoch 128 - Training loss: 0.013448249430784697\n",
            "Epoch 129 - Training loss: 0.013444982026998379\n",
            "Epoch 130 - Training loss: 0.013441813402349435\n",
            "Epoch 131 - Training loss: 0.013438821723095224\n",
            "Epoch 132 - Training loss: 0.013435949388702836\n",
            "Epoch 133 - Training loss: 0.013433177124331647\n",
            "Epoch 134 - Training loss: 0.0134306160815626\n",
            "Epoch 135 - Training loss: 0.013428095863731225\n",
            "Epoch 136 - Training loss: 0.013425705435721181\n",
            "Epoch 137 - Training loss: 0.013423405540517614\n",
            "Epoch 138 - Training loss: 0.013421230925883673\n",
            "Epoch 139 - Training loss: 0.013419090133584752\n",
            "Epoch 140 - Training loss: 0.013417096820341635\n",
            "Epoch 141 - Training loss: 0.013415122233755603\n",
            "Epoch 142 - Training loss: 0.01341327375885618\n",
            "Epoch 143 - Training loss: 0.013411473753991762\n",
            "Epoch 144 - Training loss: 0.013409734385300286\n",
            "Epoch 145 - Training loss: 0.013408068431942146\n",
            "Epoch 146 - Training loss: 0.013406453642381144\n",
            "Epoch 147 - Training loss: 0.013404903314488976\n",
            "Epoch 148 - Training loss: 0.013403397047585702\n",
            "Epoch 149 - Training loss: 0.01340196114858683\n",
            "Epoch 150 - Training loss: 0.01340055468056021\n",
            "Epoch 151 - Training loss: 0.01339922180470032\n",
            "Epoch 152 - Training loss: 0.013397891257146868\n",
            "Epoch 153 - Training loss: 0.013396628165641163\n",
            "Epoch 154 - Training loss: 0.013395397446436596\n",
            "Epoch 155 - Training loss: 0.013394224740376201\n",
            "Epoch 156 - Training loss: 0.013393066499287946\n",
            "Epoch 157 - Training loss: 0.013391943277107387\n",
            "Epoch 158 - Training loss: 0.013390860644086629\n",
            "Epoch 159 - Training loss: 0.013389805432031804\n",
            "Epoch 160 - Training loss: 0.013388788374730303\n",
            "Epoch 161 - Training loss: 0.013387786684251285\n",
            "Epoch 162 - Training loss: 0.01338682001267996\n",
            "Epoch 163 - Training loss: 0.01338589678318063\n",
            "Epoch 164 - Training loss: 0.01338497456163168\n",
            "Epoch 165 - Training loss: 0.01338409128469191\n",
            "Epoch 166 - Training loss: 0.01338321692604996\n",
            "Epoch 167 - Training loss: 0.013382381181928176\n",
            "Epoch 168 - Training loss: 0.013381557792566622\n",
            "Epoch 169 - Training loss: 0.013380766863922906\n",
            "Epoch 170 - Training loss: 0.013379980320747517\n",
            "Epoch 171 - Training loss: 0.013379231643497566\n",
            "Epoch 172 - Training loss: 0.013378490381461532\n",
            "Epoch 173 - Training loss: 0.01337777182483409\n",
            "Epoch 174 - Training loss: 0.013377073804458862\n",
            "Epoch 175 - Training loss: 0.013376382503752844\n",
            "Epoch 176 - Training loss: 0.013375715900778393\n",
            "Epoch 177 - Training loss: 0.01337506254262562\n",
            "Epoch 178 - Training loss: 0.013374427527990899\n",
            "Epoch 179 - Training loss: 0.013373805675655603\n",
            "Epoch 180 - Training loss: 0.013373194433324322\n",
            "Epoch 181 - Training loss: 0.013372598693387795\n",
            "Epoch 182 - Training loss: 0.013372020371541192\n",
            "Epoch 183 - Training loss: 0.013371455135366207\n",
            "Epoch 184 - Training loss: 0.013370889934557902\n",
            "Epoch 185 - Training loss: 0.013370352443543416\n",
            "Epoch 186 - Training loss: 0.013369819031486029\n",
            "Epoch 187 - Training loss: 0.013369298817094746\n",
            "Epoch 188 - Training loss: 0.013368789253968604\n",
            "Epoch 189 - Training loss: 0.013368290318529817\n",
            "Epoch 190 - Training loss: 0.01336780065505565\n",
            "Epoch 191 - Training loss: 0.01336732716004871\n",
            "Epoch 192 - Training loss: 0.013366857136870865\n",
            "Epoch 193 - Training loss: 0.013366405404161048\n",
            "Epoch 194 - Training loss: 0.01336594908557172\n",
            "Epoch 195 - Training loss: 0.013365509583838756\n",
            "Epoch 196 - Training loss: 0.013365080562431979\n",
            "Epoch 197 - Training loss: 0.013364658266588857\n",
            "Epoch 198 - Training loss: 0.013364240468208548\n",
            "Epoch 199 - Training loss: 0.013363838820612128\n",
            "\n",
            "Training Time (in minutes) = 0.5656956553459167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqZLzMbbWivj",
        "colab_type": "code",
        "outputId": "3042ee25-0c97-4e24-8283-361d530055b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "correct_count, all_count = 0, 0\n",
        "Yiter = iter(labelsTest)\n",
        "for images in testloader:\n",
        "  output = model(images.float())\n",
        "  output = output.round() \n",
        "  # print(output.round())\n",
        "  labs = Yiter.next()\n",
        "  # print(\"OP: \", output.shape)\n",
        "  # print(\"TRUTH: \", labs.shape)\n",
        "  R, C = output.shape\n",
        "  for r in range(0, R):\n",
        "    res = abs(output[r, :] - labs[r, :])\n",
        "    res = sum(res)\n",
        "    if res == 0:\n",
        "      correct_count += 1\n",
        "    all_count += 1\n",
        "  # for i in range(len(labels)):\n",
        "  #   img = images[i].view(1, 784)\n",
        "  #   with torch.no_grad():\n",
        "  #       logps = model(img)\n",
        "\n",
        "    \n",
        "  #   ps = torch.exp(logps)\n",
        "  #   probab = list(ps.numpy()[0])\n",
        "  #   pred_label = probab.index(max(probab))\n",
        "  #   true_label = labels.numpy()[i]\n",
        "  #   if(true_label == pred_label):\n",
        "  #     correct_count += 1\n",
        "  #   all_count += 1\n",
        "\n",
        "print(\"Number Of Samples Tested =\", all_count)\n",
        "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number Of Samples Tested = 5000\n",
            "\n",
            "Model Accuracy = 0.9032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge2B8iV4Y28L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}